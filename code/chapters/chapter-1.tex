\chapter{Introduction}

\section{The Chaos}

It all began in the early 16th century. Mathematicians were being haunted-not by demons, but by a single, innocent-looking equation. The world was changing, ideas were blossoming, and yet, one question kept echoing through the minds of the greats:

\emph{How do we solve an equation like $x^2 + 1 = 0$?}

Girolamo Cardano, one of the pioneering minds of the Renaissance, while working on the theory of polynomials, stumbled upon this ghostly riddle. The question sounded deceptively simple:

\begin{center}
\emph{"Does there exist a number which, when squared, gives the negative of one?"}
\end{center}

But why was the curiosity so deeply tied to \emph{unity}, i.e., $1$? Simple-because if we could tame $x^2 + 1 = 0$, we could solve $x^2 + a = 0$ for any real $a$. Mathematicians knew that if some mysterious number $i$ satisfied

\[
i^2 = -1,
\]

then automatically,

\[
(ix)^2 = -x^2 \quad \text{for any } x \in \mathbb{R}.
\]

In short, solve it once for $1$, and you solve it for everything.

But this wasn’t just a puzzle-it was a crisis. The number $i$ didn’t live in the world of real numbers. It belonged to a realm yet undefined. Mathematicians at the time were bound to the real line. They didn’t know there was a whole new dimension waiting for them.

And then, in 1572, a spark appeared.

\subsection*{Rafael Bombelli: The Brave Alchemist of Numbers}

Bombelli, a fearless Italian mathematician, trusted algebra like a friend. He did the unthinkable:

\[
\sqrt{-1} \cdot \sqrt{-1} = -1.
\]

It seems so obvious now-but back then, this was heresy! Why hadn’t anyone else written it? Because mathematics demands not courage, but \emph{rigour}. The square root operation was never defined for negative numbers, and jumping ahead could mean logical disaster. Yet Bombelli took that leap-not carelessly, but carefully-opening the door to a whole new world.

\subsection*{Wessel and Argand: The Visionaries}

Later, in the late 18th and early 19th centuries, two legends-Caspar Wessel (1797) and Jean-Robert Argand (1806)-entered the scene. They didn’t just accept the existence of $i$; they visualized it.

They realized something profound:

- If real numbers like $1$, $2$, $\sqrt{2}$ exist,
- And imaginary numbers like $i$, $2i$, $\sqrt{2}i$ exist,
- Then numbers of the form $a + bi$ must exist too.

These became known as **Complex Numbers**, denoted by $\mathbb{C}$.

And here came the marvel-they saw multiplication not just as arithmetic, but as **scaling and rotation**.

\subsection*{The Magic of $i$}

They studied this bizarre identity:

\[
i \cdot i = -1.
\]

Geometrically, this meant: starting at $1$, multiply by $i$ once-and you turn 90° counterclockwise. Multiply again-and you've made a half-turn. You’ve reached $-1$.

\begin{center}
\begin{tikzpicture}[scale=2]

  % Draw the semicircle
  \draw[thick, ->, postaction={decorate},
        decoration={markings, mark=at position 0.6 with {\arrow{>}}}]
    (1,0) arc (0:180:1);

  % Draw the diameter (optional, for clarity)
  \draw[dashed] (-1,0) -- (1,0);

  % Points on ends
  \filldraw (-1,0) circle (0.03);
  \filldraw (1,0) circle (0.03);

  % Labels
  \node[below] at (-1,0) {$-1$};
  \node[below] at (1,0) {$1$};

\end{tikzpicture}

\textit{Multiplying by $i$ twice is a rotation by $\pi$ radians.}
\end{center}

So naturally, they asked:

\begin{quote}
“If multiplying by $i$ twice rotates by $\pi$, then multiplying once must rotate by $\frac{\pi}{2}$, right?”
\end{quote}

This thought ignited the idea of a **two-dimensional number line**.

\subsection*{The Birth of the Argand Plane}

\begin{center}
\begin{tikzpicture}[scale=1.5, >=stealth]

  % Axes
  \draw[->] (-2.2,0) -- (2.2,0) node[right] {$\Re$};
  \draw[->] (0,-2.2) -- (0,2.2) node[above] {$\Im$};

  % Points
  \filldraw[black] (1,0) circle (0.05) node[below right] {$1$};
  \filldraw[black] (-1,0) circle (0.05) node[below left] {$-1$};
  \filldraw[black] (0,1) circle (0.05) node[above right] {$i$};
  \filldraw[black] (0,-1) circle (0.05) node[below right] {$-i$};
  \filldraw[black] (0,0) circle (0.04) node[below left] {$0$};

\end{tikzpicture}
\end{center}

Every complex number $z = a + ib$ is now just a point in this plane-called the **Argand Plane**. And every such number has a natural position vector from the origin.

This laid the foundation for the majestic study of complex numbers, and later, complex-valued functions-what we today call **Complex Analysis**.

Oh, and by the way, that’s why we write $i$, $2i$, $3i$, etc., on the vertical axis not $\sqrt{-1}$, $2\sqrt{-1}$... because $i$ was designed to follow a geometric thought process. It fit the rotation; it respected the structure.

---

\section{Operations on Complex Numbers}

Now let’s explore the operations.

Let
\[
z_1 = a_1 + b_1 i, \quad z_2 = a_2 + b_2 i
\]
Then addition is simple:
\[
z_1 + z_2 = (a_1 + a_2) + (b_1 + b_2)i
\]

Multiplication is where the magic deepens:
\begin{align*}
z_1 \cdot z_2 &= (a_1 + b_1 i)(a_2 + b_2 i) \\
&= a_1 a_2 + a_1 b_2 i + b_1 a_2 i + b_1 b_2 i^2 \\
&= (a_1 a_2 - b_1 b_2) + (a_1 b_2 + b_1 a_2)i
\end{align*}

And division? Just as neat:
\begin{align*}
\frac{z_1}{z_2} &= \frac{a_1 + b_1 i}{a_2 + b_2 i} \cdot \frac{a_2 - b_2 i}{a_2 - b_2 i} \\
&= \frac{(a_1 a_2 + b_1 b_2) + (b_1 a_2 - a_1 b_2)i}{a_2^2 + b_2^2} \\
&= \frac{a_1 a_2 + b_1 b_2}{a_2^2 + b_2^2} + \frac{b_1 a_2 - a_1 b_2}{a_2^2 + b_2^2}i
\end{align*}

This tells us that the set $\mathbb{C}$ is **closed** under addition, subtraction, multiplication, and division. This is called the **closure property**-and it’s one of the first signs of $\mathbb{C}$'s algebraic power.

Similarly, we know that
\begin{align*}
z_1 \cdot z_2 &= (a_1 a_2 - b_1 b_2) + (a_1 b_2 + b_1 a_2)i
\end{align*}

Now let us reverse the order:
\begin{align*}
z_2 \cdot z_1 &= (a_2 a_1 - b_2 b_1) + (a_2 b_1 + b_2 a_1)i
\end{align*}

Similarly, let us check for addition:
\[
z_1 + z_2 = (a_1 + a_2) + (b_1 + b_2)i
\]
And after reversing the order:
\[
z_2 + z_1 = (a_2 + a_1) + (b_2 + b_1)i
\]

Since both \emph{addition} and \emph{multiplication} are commutative for complex numbers, we conclude that the set $\mathbb{C}$ is commutative under both operations.

Furthermore, one can verify that $\mathbb{C}$ is \emph{associative}, meaning the grouping of operations does not affect the result. This follows from the fact that $\mathbb{R}$ is associative under addition and multiplication.

Let us consider three complex numbers:
\begin{align*}
z_1 &= a_1 + b_1 i \\
z_2 &= a_2 + b_2 i \\
z_3 &= a_3 + b_3 i
\end{align*}

Now compute:
\begin{align*}
z_1 (z_2 + z_3) &= (a_1 + b_1 i)((a_2 + a_3) + (b_2 + b_3)i) \\
&= a_1(a_2 + a_3) - b_1(b_2 + b_3) + [a_1(b_2 + b_3) + b_1(a_2 + a_3)]i \\
&= z_1 z_2 + z_1 z_3
\end{align*}

Thus,
\[
z_1(z_2 + z_3) = z_1 z_2 + z_1 z_3
\]
This is called the \emph{left distributive property}. Similarly, we can verify:
\[
(z_2 + z_3)z_1 = z_2 z_1 + z_3 z_1
\]
which is called the \emph{right distributive property}. Hence, $\mathbb{C}$ is distributive under multiplication.

Therefore, the set $\mathbb{C}$ satisfies closure, associativity, distributivity, and commutativity-just like the set $\mathbb{R}$.

Now, returning to operations on complex numbers: we already understand that any $z \in \mathbb{C}$ can be visualized as a position vector in the Argand plane. Thus, it must have both a magnitude and an angle.

\begin{center}
\begin{tikzpicture}[scale=2, >=stealth]
  % Axes
  \draw[->, thick] (-0.5,0) -- (3.2,0) node[right] {$\Re$};
  \draw[->, thick] (0,-0.5) -- (0,2.5) node[above] {$\Im$};

  % Coordinates
  \coordinate (O) at (0,0);
  \coordinate (Z) at (2,1.5);

  % Point z = a + bi
  \filldraw[black] (Z) circle (0.05) node[above right] {$z = a + bi$};

  % Dotted lines
  \draw[dashed, black] (Z) -- (2,0) node[below] {$a$};
  \draw[dashed, black] (Z) -- (0,1.5) node[left] {$bi$};

  % Position vector
  \draw[->, thick, black] (O) -- (Z);

  % Origin
  \filldraw[black] (O) circle (0.04) node[below left] {$0$};

  % Angle theta arc
  \draw[->, thick] (1.2,0) arc[start angle=0, end angle=36.87, radius=1.2];
  \node at (0.8,0.25) {$\theta$};
\end{tikzpicture}
\end{center}

Let the real part of $z$ be denoted by $\operatorname{Re}(z)$ and the imaginary part by $\operatorname{Im}(z)$. Let $R$ denote the magnitude of the vector $z$. Then using basic trigonometry:

\begin{align}
\sin \theta &= \frac{\operatorname{Im}(z)}{R} \tag{1.1} \\
\cos \theta &= \frac{\operatorname{Re}(z)}{R} \tag{1.2}
\end{align}

Since
\[
z = \operatorname{Re}(z) + \operatorname{Im}(z) \cdot i,
\]
multiplying both sides of (1.1) and (1.2) by $R$ gives:
\begin{align}
z = R\left( \cos \theta + i \sin \theta \right) \tag{1.3}
\end{align}

Equation (1.3) is the polar form of a complex number, a new and insightful representation of $z \in \mathbb{C}$.

\subsection*{The tale of Polynomial Representations}

This chapter serves as an introduction and does not delve deeply into the subject, but for the sake of \emph{order in this book}, we shall touch upon the concept of the Taylor expansion.

\vspace{1em}
When calculus was formalized, mathematicians began to wonder how one might approximate a curve through simpler expressions. They sought various representations of functions that could reveal hidden properties-representations that could decode the behavior of functions through the lens of polynomials.

\vspace{1em}
Newton, with his method of finite differences, had already shown a striking expansion:
\[
(1 + x)^n = 1 + nx + \frac{n(n-1)x^2}{2!} + \frac{n(n-1)(n-2)x^3}{3!} + \dotsb
\]

This expansion ignited a flame-a desire to uncover whether other functions could be expressed similarly, as infinite series of polynomial terms. This quest eventually led to the discovery of the Taylor series.

\vspace{1em}
Thus arose the question: \emph{Can any function be represented as a power series centered around a point?} This led to the formulation of the Taylor series, which we shall explore in depth in later chapters. For now, let us state the definition:

\begin{quote}
\emph{The Taylor series of a function $f(x)$, expanded around a point $a$, is given by}
\[
f(x) = \sum_{i=0}^{\infty} \frac{f^{(i)}(a)}{i!} (x - a)^i = f(a) + f'(a)(x-a) + \frac{f''(a)}{2!}(x-a)^2 + \dotsb
\]
\end{quote}

\vspace{1em}
When we say a Taylor series is \emph{centered at $a$}, we mean that the entire function is reconstructed from its derivatives evaluated at $x = a$. However, there are crucial conditions under which this reconstruction faithfully represents the function, and when it does not. We will explore those subtleties in detail later.

For now, let us focus on a special case: the \emph{Maclaurin series}, which is simply the Taylor series centered at $a = 0$. Remarkably, the Maclaurin series expansions of $\sin \theta$, $\cos \theta$, and $e^x$ converge for all complex values $x \in \mathbb{C}$.

\vspace{1em}
Let us derive the Maclaurin series for $\sin x$:
\begin{align*}
\sin x &= \sin 0 + \sin'(0)\cdot x + \frac{\sin''(0)}{2!}x^2 + \frac{\sin^{(3)}(0)}{3!}x^3 + \dotsb \\
&= 0 + 1\cdot x - 0\cdot x^2 - \frac{1}{3!}x^3 + 0\cdot x^4 + \frac{1}{5!}x^5 - \dotsb \\
&= x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + \dotsb
\end{align*}

Similarly, for $\cos x$:
\begin{align*}
\cos x &= \cos 0 + \cos'(0)\cdot x + \frac{\cos''(0)}{2!}x^2 + \dotsb \\
&= 1 + 0\cdot x - \frac{1}{2!}x^2 + 0\cdot x^3 + \frac{1}{4!}x^4 - \dotsb \\
&= 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \frac{x^6}{6!} + \dotsb
\end{align*}

And for $e^x$, since all derivatives of $e^x$ are again $e^x$, we get:
\[
e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \frac{x^4}{4!} + \dotsb
\]

\vspace{1em}
These Maclaurin series unlocked a gateway-a bridge between the polar form of complex numbers and Euler's constant $e \approx 2.718\ldots$. It all began when Euler observed:

\begin{quote}
"The expansions of $\sin x$ and $\cos x$, when added, seem strangely close to the expansion of $e^x$."
\end{quote}

Yet the sign pattern did not align. Consider:
\begin{align}
\sin x + \cos x &= 1 + x - \frac{x^2}{2!} - \frac{x^3}{3!} + \frac{x^4}{4!} + \frac{x^5}{5!} - \frac{x^6}{6!} - \frac{x^7}{7!} + \dotsb \tag{1.4}
\end{align}

Neither addition nor subtraction of the two series fixed the chaotic sign scheme. Then came the epiphany.

\vspace{1em}
Euler considered the powers of $i$:
\[
i^0 = 1,\quad i^1 = i,\quad i^2 = -1,\quad i^3 = -i,\quad i^4 = 1,\quad \dotsb
\]

The sign pattern matched perfectly with the alternating structure of complex series! Inspired, Euler considered the expansion of $e^{ix}$:
\begin{align*}
e^{ix} &= 1 + ix + \frac{(ix)^2}{2!} + \frac{(ix)^3}{3!} + \frac{(ix)^4}{4!} + \dotsb \\
&= 1 + ix - \frac{x^2}{2!} - i\frac{x^3}{3!} + \frac{x^4}{4!} + i\frac{x^5}{5!} - \dotsb \tag{1.5}
\end{align*}

Then Euler did something magical. He grouped the real and imaginary parts:
\[
e^{ix} = \left(1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \dotsb\right) + i\left(x - \frac{x^3}{3!} + \frac{x^5}{5!} - \dotsb\right)
\]

And there it was-the spark of genius:
\[
e^{ix} = \cos x + i \sin x \tag{1.6}
\]

This elegant identity, now known as Euler’s formula, bridges exponential and trigonometric functions through the gateway of the complex plane.

\vspace{1em}
Let us now return to geometry. Recall the polar form of a complex number $z \in \mathbb{C}$, with modulus $R = |z|$ and argument $\theta$ (the angle with the real axis). Euler gave it a new life:

\begin{quote}
Every complex number can be expressed in exponential polar form:
\[
z = R \cdot (\cos \theta + i \cdot \sin \theta) = R \cdot e^{i\theta} = |z| e^{i\theta}
\]
\end{quote}

This was a golden moment in mathematics. A bridge was built-seamlessly connecting trigonometry, complex numbers, and the exponential world.

\section*{Exponents in Complex Numbers}

A natural question arises when we enter the complex realm:

\begin{center}
    \emph{How do we define and compute exponents in} $\mathbb{C}$?
\end{center}

More precisely:  
\begin{center}
    \emph{How do we evaluate $z^n$, where $z \in \mathbb{C}$ and $n \in \mathbb{Z}$?}
\end{center}

To answer this, a French mathematician, \textbf{Abraham De Moivre}, stepped into the scene and offered an elegant insight.

He began with the polar representation of a complex number:
\[
z = \cos \theta + i \sin \theta
\]
But noticing Euler’s identity, he realized this can also be written as:
\[
z = e^{i\theta}
\]
Hence, raising $z$ to the power $n$ gives:
\[
z^n = \left(e^{i\theta}\right)^n = e^{i n\theta}
\]
Expanding this back to trigonometric form:
\begin{align*}
z^n = (\cos \theta + i \sin \theta)^n = e^{i n\theta} = \cos(n\theta) + i \sin(n\theta)
\end{align*}

He thus concluded the identity:
\begin{align}
(\cos \theta + i \sin \theta)^n = \cos(n\theta) + i \sin(n\theta)
\label{eq:demoivre}
\end{align}

Equation \eqref{eq:demoivre} is now famously known as \textbf{De Moivre's Theorem}.  
But this elegant result can also be proven by mathematical induction.

---

\textbf{Proof by Induction:}

Assume for some integer $k$, the identity holds:
\[
(\cos \theta + i \sin \theta)^k = \cos(k\theta) + i \sin(k\theta)
\]
Then consider:
\begin{align*}
(\cos \theta + i \sin \theta)^{k+1} 
&= (\cos(k\theta) + i \sin(k\theta))(\cos \theta + i \sin \theta) \\
&= \cos(k\theta)\cos\theta - \sin(k\theta)\sin\theta \\
&\quad + i[\cos(k\theta)\sin\theta + \sin(k\theta)\cos\theta] \\
&= \cos((k+1)\theta) + i \sin((k+1)\theta)
\end{align*}

Thus, if the identity holds for $k$, it holds for $k+1$.  
Since the base case:
\[
(\cos \theta + i \sin \theta)^1 = \cos \theta + i \sin \theta
\]
is trivially true, it follows by induction that the identity holds for all $n \in \mathbb{N}$.

Hence, by \textbf{De Moivre’s Theorem}, we can give precise meaning to expressions like $z^n$ in the complex plane, beautifully connecting exponents, trigonometry, and Euler’s insight.
